{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from lxml import etree\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import Column, Integer, String\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from pydantic import ValidationError\n",
    "from sqlalchemy import Sequence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ORM Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6q/q891flcj0r375hpjwrt2wtbm0000gn/T/ipykernel_12910/4149098369.py:1: MovedIn20Warning: Deprecated API features detected! These feature(s) are not compatible with SQLAlchemy 2.0. To prevent incompatible upgrades prior to updating applications, ensure requirements files are pinned to \"sqlalchemy<2.0\". Set environment variable SQLALCHEMY_WARN_20=1 to show all deprecation warnings.  Set environment variable SQLALCHEMY_SILENCE_UBER_WARNING=1 to silence this message. (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
      "  Base = declarative_base()\n"
     ]
    }
   ],
   "source": [
    "Base = declarative_base()\n",
    "\n",
    "class MetaDataOrm(Base):\n",
    "    __tablename__ = 'metadata'\n",
    "\n",
    "    id = Column(Integer, Sequence('metadata_table_id_seq'), primary_key=True, autoincrement=True)\n",
    "    title = Column(String)\n",
    "    publisher = Column(String, default=2023)\n",
    "    availability_status = Column(String, default=\"\")\n",
    "    analytic = Column(String, default=\"Not Available\")\n",
    "    imprinted_date = Column(String)\n",
    "    abstract = Column(String, default=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "user = os.getenv('SNOWFLAKE_USER')\n",
    "password = os.getenv('SNOWFLAKE_PASSWORD')\n",
    "account = os.getenv('SNOWFLAKE_ACCOUNT')\n",
    "warehouse = os.getenv('SNOWFLAKE_WAREHOUSE')\n",
    "database = os.getenv('SNOWFLAKE_DATABASE')\n",
    "schema = os.getenv('SNOWFLAKE_SCHEMA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Data from XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(input_path):\n",
    "    \"\"\"\n",
    "    Extracts metadata from an XML file using XPath expressions and appends the information to a list.\n",
    "\n",
    "    Parameters:\n",
    "    - input_path (str): The path to the XML file.\n",
    "    - pdf_content_list (list): A list to which extracted metadata dictionaries will be appended.\n",
    "    - bucket_links (dict): A dictionary containing links to S3 buckets corresponding to XML files.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Get the absolute path of the XML file\n",
    "    xml_file_path = os.path.abspath(input_path)\n",
    "    # Parse the XML file\n",
    "    if os.path.exists(xml_file_path):\n",
    "        tree = etree.parse(xml_file_path)\n",
    "        root = tree.getroot()\n",
    "        # Define XML namespaces\n",
    "        namespaces = {\n",
    "            'tei': 'http://www.tei-c.org/ns/1.0',\n",
    "            'xlink': 'http://www.w3.org/1999/xlink'\n",
    "        }\n",
    "\n",
    "        def get_first_item(xpath_result):\n",
    "            \"\"\"\n",
    "            Helper function to get the first item from an XPath result.\n",
    "\n",
    "            Parameters:\n",
    "            - xpath_result (list): List of XPath results.\n",
    "\n",
    "            Returns:\n",
    "            str: The first item or \"No Data\" if the list is empty.\n",
    "            \"\"\"\n",
    "            if xpath_result:\n",
    "                # Remove newline and tab characters and return the first item\n",
    "                xpath_result[0] = xpath_result[0].replace('\\n', '').replace('\\t','')\n",
    "                return f\"{xpath_result[0]}\"  \n",
    "            else:\n",
    "                return \"Not Available\"\n",
    "\n",
    "        # Extract metadata using XPath expressions\n",
    "        metadata_dict = {\n",
    "            \"Title\": get_first_item(root.xpath('//tei:titleStmt/tei:title[@level=\"a\" and @type=\"main\"]/text()', namespaces=namespaces)),\n",
    "            \"Publisher\": get_first_item(root.xpath('//tei:publicationStmt/tei:publisher/text()', namespaces=namespaces)),\n",
    "            \"AvailabilityStatus\": get_first_item(root.xpath('//tei:availability/@status', namespaces=namespaces)),\n",
    "            \"Analytic\": get_first_item(root.xpath('//tei:analytic/text()', namespaces=namespaces)),\n",
    "            \"ImprintedDate\": get_first_item(root.xpath('//tei:imprint/tei:date/text()', namespaces=namespaces)),\n",
    "            \"AppInfoDescription\": get_first_item(root.xpath('//tei:application/tei:desc/text()', namespaces=namespaces)),\n",
    "            \"Abstract\": get_first_item(root.xpath('//tei:profileDesc/tei:abstract/tei:p/text()', namespaces=namespaces)),\n",
    "        }\n",
    "        # Append the metadata dictionary to the list\n",
    "        return metadata_dict\n",
    "    else:\n",
    "        print(f\"The file {xml_file_path} does not exist.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Csv and transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6q/q891flcj0r375hpjwrt2wtbm0000gn/T/ipykernel_12910/312732867.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "raw_csv_path = '../resources/grobid_xml_data/raw_metadata.csv'\n",
    "def extract_content(paths,raw_csv_path):\n",
    "    pdf_content_list=[]\n",
    "    for path in paths:\n",
    "        pdf_content_list.append(extract(path))\n",
    "\n",
    "    md = pd.DataFrame(pdf_content_list)\n",
    "    \n",
    "    md.to_csv(raw_csv_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = ['../resources/grobid_xml_data/Level1_combined.grobid.tei.xml',\n",
    "         '../resources/grobid_xml_data/Level2_combined.grobid.tei.xml',\n",
    "         '../resources/grobid_xml_data/Level3_combined.grobid.tei.xml',\n",
    "        ]\n",
    "extract_content(paths,raw_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6q/q891flcj0r375hpjwrt2wtbm0000gn/T/ipykernel_12910/1758427842.py:4: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  md = md.applymap(lambda x: x.replace('\\n', '').replace('\\t', '') if isinstance(x, str) else x)\n"
     ]
    }
   ],
   "source": [
    "def process_data(input_path,processed_csv_path):\n",
    "    md = pd.read_csv(input_path)\n",
    "    md = md.fillna('Not Available')\n",
    "    md = md.applymap(lambda x: x.replace('\\n', '').replace('\\t', '') if isinstance(x, str) else x)\n",
    "    md.to_csv(processed_csv_path, index=False)\n",
    "\n",
    "processed_csv_path = '../resources/clean_csv/processed_metadata.csv'\n",
    "\n",
    "process_data(raw_csv_path,processed_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create ORM Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_orm_instances(df):\n",
    "    orm_instances = []\n",
    "    for _, row in df.iterrows():\n",
    "        orm_instance = MetaDataOrm(\n",
    "                title=row['Title'],\n",
    "                publisher= row['Publisher'],\n",
    "                analytic=row['Analytic'],\n",
    "                imprinted_date=row['ImprintedDate'],\n",
    "                abstract=row['AppInfoDescription'],\n",
    "                availability_status=row['Abstract']\n",
    "        )\n",
    "        orm_instances.append(orm_instance)\n",
    "    return orm_instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Pydantic to validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6q/q891flcj0r375hpjwrt2wtbm0000gn/T/ipykernel_12910/601115387.py:11: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.6/migration/\n",
      "  @validator('title', 'publisher', 'availability_status', 'analytic', 'imprinted_date', 'abstract', pre=True, each_item=False)\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field, validator, ValidationError\n",
    "\n",
    "class MetaData(BaseModel):\n",
    "    title: str = Field(alias='Title', default=\"Unknown\")  \n",
    "    publisher: str = Field(default=\"Unknown\")\n",
    "    availability_status: str = Field(default=\"\", alias='availability_status')\n",
    "    analytic: str = Field(default=\"Not Available\", alias='analytic', min_length=2)\n",
    "    imprinted_date: str = Field(..., alias='imprinted_date')  \n",
    "    abstract: str = Field(default=\"\", min_length=2)\n",
    "\n",
    "    @validator('title', 'publisher', 'availability_status', 'analytic', 'imprinted_date', 'abstract', pre=True, each_item=False)\n",
    "    def check_empty_string(cls, v,values):\n",
    "        if v == \"\":\n",
    "            raise ValueError(f'empty string not allowed')\n",
    "\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, validator, ValidationError\n",
    "\n",
    "def orm_instance_to_meta_pydantic(orm_instance):\n",
    "    return MetaData(**orm_instance.__dict__)\n",
    "def convert_to_pydantic_instances(orm_instances):\n",
    "        return [orm_instance_to_meta_pydantic(orm_instance) for orm_instance in orm_instances]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create orm instances from csv and then validate with pydantic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_orm_instances_and_validate_using_pydantic(path):\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        orm_instances = create_orm_instances(df)\n",
    "        for i in orm_instances:\n",
    "            print(i.title)\n",
    "        pydantic_instances = convert_to_pydantic_instances(orm_instances)\n",
    "        print(f\"{len(pydantic_instances)} are validated\")\n",
    "        return orm_instances\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        print(\"Error in validation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Available\n",
      "Not Available\n",
      "2024 Level III Topic Outlines Economics LEARNING OUTCOMES Capital Market Expectations, Part 1: Framework and Macro Considerations\n",
      "3 are validated\n"
     ]
    }
   ],
   "source": [
    "orm_instances = create_orm_instances_and_validate_using_pydantic(processed_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload to snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_database_if_not_exists(engine, database):\n",
    "    connection = engine.connect()\n",
    "    connection.execute(\"CREATE DATABASE IF NOT EXISTS {}\".format(database))\n",
    "    # connection.execute(\"USE schema cfa_rr_list\")\n",
    "    connection.close()\n",
    "def upload_to_snowflake(engine, orm_instances, database):\n",
    "        create_database_if_not_exists(engine, database)\n",
    "        Base.metadata.bind = engine\n",
    "        if not engine.dialect.has_table(engine, MetaDataOrm.__tablename__):\n",
    "            MetaDataOrm.__table__.create(bind=engine)\n",
    "        else:\n",
    "            print(f\"Table '{MetaDataOrm.__tablename__}' already exists.\")\n",
    "        \n",
    "        SessionClass = sessionmaker(bind= engine)\n",
    "        session = SessionClass()\n",
    "\n",
    "        # Get the total number of ORM instances to insert\n",
    "        total_instances = len(orm_instances)\n",
    "\n",
    "        # Set a threshold for printing progress updates (adjust as needed)\n",
    "        progress_threshold = 1\n",
    "\n",
    "        for idx, orm_instance in enumerate(orm_instances, start=1):\n",
    "            session.add(orm_instance)\n",
    "            print(orm_instance.id)\n",
    "            # Commit the changes periodically to the database\n",
    "            if idx % progress_threshold == 0:\n",
    "                session.commit()\n",
    "                print(f\"Inserted {idx}/{total_instances} records.\")\n",
    "\n",
    "        # Commit any remaining changes\n",
    "        session.commit()\n",
    "        print(f\"Inserted {total_instances}/{total_instances} records.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Inserted 1/3 records.\n",
      "None\n",
      "Inserted 2/3 records.\n",
      "None\n",
      "Inserted 3/3 records.\n",
      "Inserted 3/3 records.\n"
     ]
    }
   ],
   "source": [
    "engine = create_engine(\n",
    "    f'snowflake://{user}:{password}@{account}/{database}/{schema}?warehouse={warehouse}'\n",
    ")\n",
    "upload_to_snowflake(engine=engine, orm_instances=orm_instances, database=database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
