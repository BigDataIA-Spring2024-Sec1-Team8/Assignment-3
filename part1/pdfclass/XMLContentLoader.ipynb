{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from lxml import etree\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import Column, Integer, String\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from pydantic import ValidationError\n",
    "from sqlalchemy import Sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6q/q891flcj0r375hpjwrt2wtbm0000gn/T/ipykernel_13089/491773305.py:1: MovedIn20Warning: Deprecated API features detected! These feature(s) are not compatible with SQLAlchemy 2.0. To prevent incompatible upgrades prior to updating applications, ensure requirements files are pinned to \"sqlalchemy<2.0\". Set environment variable SQLALCHEMY_WARN_20=1 to show all deprecation warnings.  Set environment variable SQLALCHEMY_SILENCE_UBER_WARNING=1 to silence this message. (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
      "  Base = declarative_base()\n"
     ]
    }
   ],
   "source": [
    "Base = declarative_base()\n",
    "\n",
    "class LearningOutcomesOrm(Base):\n",
    "    __tablename__ = 'learning_outcomes_table'\n",
    "\n",
    "    id = Column(Integer, Sequence('learning_outcomes_table_id_seq'), primary_key=True, autoincrement=True)\n",
    "    topic = Column(String)\n",
    "    outcomes = Column(String, default=2023)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "user = os.getenv('SNOWFLAKE_USER')\n",
    "password = os.getenv('SNOWFLAKE_PASSWORD')\n",
    "account = os.getenv('SNOWFLAKE_ACCOUNT')\n",
    "warehouse = os.getenv('SNOWFLAKE_WAREHOUSE')\n",
    "database = os.getenv('SNOWFLAKE_DATABASE')\n",
    "schema = os.getenv('SNOWFLAKE_SCHEMA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import xml.etree.ElementTree as ET\n",
    "def create_outcomes(csv_filename, xml_file_paths):\n",
    "    with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        \n",
    "        # Write header\n",
    "        csv_writer.writerow(['Topic', 'Learning_Outcomes'])\n",
    "        \n",
    "        # Iterate through each XML file\n",
    "        for xml_file_path in xml_file_paths:\n",
    "            print(xml_file_path)\n",
    "            # Read the XML data from the file\n",
    "            with open(xml_file_path, 'r') as file:\n",
    "                xml_data = file.read()\n",
    "            # Parse the XML data\n",
    "            root = ET.fromstring(xml_data)\n",
    "\n",
    "            for div_element in root.findall('.//{http://www.tei-c.org/ns/1.0}div'):\n",
    "                    head_element = div_element.find('.//{http://www.tei-c.org/ns/1.0}head')\n",
    "\n",
    "                    p_elements = div_element.findall('.//{http://www.tei-c.org/ns/1.0}p')\n",
    "\n",
    "                    combined_p_text = ' '.join(p_element.text for p_element in p_elements if p_element.text)\n",
    "                    if combined_p_text != '':\n",
    "                        csv_writer.writerow([head_element.text if head_element is not None else '', combined_p_text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../resources/grobid_xml_data/Level1_combined.grobid.tei.xml\n",
      "../resources/grobid_xml_data/Level2_combined.grobid.tei.xml\n",
      "../resources/grobid_xml_data/Level3_combined.grobid.tei.xml\n"
     ]
    }
   ],
   "source": [
    "raw_file_path = '../resources/grobid_xml_data/raw_pdf_content.csv'\n",
    "create_outcomes(csv_filename=raw_file_path, xml_file_paths=['../resources/grobid_xml_data/Level1_combined.grobid.tei.xml',\n",
    "         '../resources/grobid_xml_data/Level2_combined.grobid.tei.xml',\n",
    "         '../resources/grobid_xml_data/Level3_combined.grobid.tei.xml',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6q/q891flcj0r375hpjwrt2wtbm0000gn/T/ipykernel_13089/128821988.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "def remove_extra_whitespaces(value):\n",
    "    if isinstance(value, str):\n",
    "        return ' '.join(value.split())\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "def pre_process_text(path, output_path):\n",
    "    df = pd.read_csv(path, dtype=str)\n",
    "    df=df.fillna('Not Available')\n",
    "    df = df.map(remove_extra_whitespaces)\n",
    "    df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_file_path = '../resources/clean_csv/processed_pdf_content.csv'\n",
    "\n",
    "pre_process_text(raw_file_path, processed_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_orm_instances(df):\n",
    "    orm_instances = []\n",
    "    for _, row in df.iterrows():\n",
    "        orm_instance = LearningOutcomesOrm(\n",
    "                topic=row['Topic'], \n",
    "                outcomes=row['Learning_Outcomes']\n",
    "        )\n",
    "        orm_instances.append(orm_instance)\n",
    "    return orm_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6q/q891flcj0r375hpjwrt2wtbm0000gn/T/ipykernel_13089/2748408219.py:11: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.6/migration/\n",
      "  @validator('topic', 'outcomes', pre=True, each_item=False)\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field, validator, ValidationError\n",
    "from typing_extensions import Annotated\n",
    "from pydantic import BaseModel, ConfigDict, Field, HttpUrl, constr\n",
    "from typing import Optional, Any\n",
    "from pydantic.functional_validators import field_validator\n",
    "from typing_extensions import Annotated\n",
    "class LearningOutcomes(BaseModel):\n",
    "    topic: str = Field(..., alias='topic', min_length=2)  \n",
    "    outcomes: str = Field(default=\"Not defined\", min_length=2)\n",
    "\n",
    "    @validator('topic', 'outcomes', pre=True, each_item=False)\n",
    "    def strip_whitespace(cls, v):\n",
    "        if isinstance(v, str):\n",
    "            ValueError('empty not allowed')\n",
    "        return v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, validator, ValidationError\n",
    "\n",
    "def orm_instance_to_pydantic(orm_instance):\n",
    "    # print(orm_instance.__dict__)\n",
    "    return LearningOutcomes(**orm_instance.__dict__)\n",
    "def convert_to_pydantic_instances(orm_instances):\n",
    "        return [orm_instance_to_pydantic(orm_instance) for orm_instance in orm_instances]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_orm_instances_and_validate_using_pydantic(path):\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        orm_instances = create_orm_instances(df)\n",
    "        pydantic_instances = convert_to_pydantic_instances(orm_instances)\n",
    "        print(f\"{len(pydantic_instances)} are validated\")\n",
    "        return orm_instances\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        print(\"Error in validation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170 are validated\n"
     ]
    }
   ],
   "source": [
    "orm_instances = create_orm_instances_and_validate_using_pydantic(processed_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_database_if_not_exists(engine, database):\n",
    "    connection = engine.connect()\n",
    "    connection.execute(\"CREATE DATABASE IF NOT EXISTS {}\".format(database))\n",
    "    connection.close()\n",
    "def upload_to_snowflake(engine, orm_instances, database):\n",
    "        create_database_if_not_exists(engine, database)\n",
    "        Base.metadata.bind = engine\n",
    "        if not engine.dialect.has_table(engine, LearningOutcomesOrm.__tablename__):\n",
    "            LearningOutcomesOrm.__table__.create(bind=engine)\n",
    "        else:\n",
    "            print(f\"Table '{LearningOutcomesOrm.__tablename__}' already exists.\")\n",
    "        \n",
    "        SessionClass = sessionmaker(bind= engine)\n",
    "        session = SessionClass()\n",
    "\n",
    "        # Get the total number of ORM instances to insert\n",
    "        total_instances = len(orm_instances)\n",
    "\n",
    "        # Set a threshold for printing progress updates (adjust as needed)\n",
    "        progress_threshold = 10\n",
    "\n",
    "        for idx, orm_instance in enumerate(orm_instances, start=1):\n",
    "            session.add(orm_instance)\n",
    "            # Commit the changes periodically to the database\n",
    "            if idx % progress_threshold == 0:\n",
    "                session.commit()\n",
    "                print(f\"Inserted {idx}/{total_instances} records.\")\n",
    "\n",
    "        # Commit any remaining changes\n",
    "        session.commit()\n",
    "        print(f\"Inserted {total_instances}/{total_instances} records.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 10/170 records.\n",
      "Inserted 20/170 records.\n",
      "Inserted 30/170 records.\n",
      "Inserted 40/170 records.\n",
      "Inserted 50/170 records.\n",
      "Inserted 60/170 records.\n",
      "Inserted 70/170 records.\n",
      "Inserted 80/170 records.\n",
      "Inserted 90/170 records.\n",
      "Inserted 100/170 records.\n",
      "Inserted 110/170 records.\n",
      "Inserted 120/170 records.\n",
      "Inserted 130/170 records.\n",
      "Inserted 140/170 records.\n",
      "Inserted 150/170 records.\n",
      "Inserted 160/170 records.\n",
      "Inserted 170/170 records.\n",
      "Inserted 170/170 records.\n"
     ]
    }
   ],
   "source": [
    "engine = create_engine(\n",
    "    f'snowflake://{user}:{password}@{account}/{database}/{schema}?warehouse={warehouse}'\n",
    ")\n",
    "upload_to_snowflake(engine=engine, orm_instances=orm_instances, database=database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "4c8b357a39b9cfa652ce5eb494eea21f40e33742edb285e95e5860972a45bd4f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
